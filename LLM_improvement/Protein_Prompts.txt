{"instruction": "Create a network with 4 layers: the first is a convolutional layer with 32 filters of size 3x3, followed by a max pooling layer, then a dense layer with 128 units and ReLU activation, and finally an output layer with 1 unit and sigmoid activation.", "response": "network: - layer_func: ~tf.layers.conv2d arguments: name: conv1 inputs: ~layers[-1] filters: 32 kernel_size: [3, 3] activation: ~tf.nn.relu - layer_func: ~tf.layers.max_pooling2d arguments: name: pool1 inputs: ~layers[-1] pool_size: [2, 2] strides: 2 - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: output inputs: ~layers[-1] units: 1 activation: ~tf.nn.sigmoid"}
{"instruction": "Define a deep network with 5 layers. Start with a dense layer of 512 units and ReLU activation, followed by another dense layer of 256 units and sigmoid activation. The next layer should have 128 units and tanh activation, followed by a dense layer of 64 units with leaky ReLU activation. The last layer should have 1 unit with softmax activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.sigmoid - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 128 activation: ~tf.nn.tanh - layer_func: ~tf.layers.dense arguments: name: dense4 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dense arguments: name: output inputs: ~layers[-1] units: 1 activation: ~tf.nn.softmax"}
{"instruction": "Create a network with 3 layers: a dense layer with 1024 units and ReLU activation, followed by a dropout layer with a rate of 0.4, and a dense layer with 256 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 2 layers: a convolutional layer with 32 filters and a kernel size of 3x3, followed by a max pooling layer with a pool size of 2x2.", "response": "network: - layer_func: ~tf.layers.conv2d arguments: name: conv1 inputs: ~layers[-1] filters: 32 kernel_size: [3, 3] - layer_func: ~tf.layers.max_pooling2d arguments: name: maxpool1 inputs: ~layers[-1] pool_size: [2, 2] strides: 2"}
{"instruction": "Create a network with 2 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with a rate of 0.5.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 4 layers: a dense layer with 128 units and ReLU activation, followed by a convolutional layer with 64 filters and a kernel size of 5x5, followed by a dropout layer with a rate of 0.3, and a dense layer with 64 units and sigmoid activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.conv2d arguments: name: conv1 inputs: ~layers[-1] filters: 64 kernel_size: [5, 5] - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 64 activation: ~tf.nn.sigmoid"}
{"instruction": "Create a network with 2 layers: a dense layer with 256 units and LeakyReLU activation, followed by a dropout layer with a rate of 0.2.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 3 layers: a convolutional layer with 16 filters and a kernel size of 3x3, followed by a dense layer with 128 units and ReLU activation, followed by a dropout layer with a rate of 0.4.", "response": "network: - layer_func: ~tf.layers.conv2d arguments: name: conv1 inputs: ~layers[-1] filters: 16 kernel_size: [3, 3] - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 3 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with a rate of 0.6, and a dense layer with 128 units and sigmoid activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.6 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.sigmoid"}
{"instruction": "Create a network with 2 layers: a dense layer with 64 units and LeakyReLU activation, followed by a dropout layer with a rate of 0.5.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 2 layers: a dense layer with 256 units and sigmoid activation, followed by a dropout layer with a rate of 0.2.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.sigmoid - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 4 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with a rate of 0.3, then a dense layer with 256 units and LeakyReLU activation, and finally a dense layer with 128 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu"}
{"instruction": "Build a network with 5 layers: a dense layer with 1024 units and ReLU activation, followed by a dropout layer with 0.5 rate, another dense layer with 512 units and LeakyReLU activation, then a dense layer with 256 units and ReLU activation, and finally a dense layer with 128 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense4 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a neural network with 2 layers: a dense layer with 128 units and ReLU activation, followed by a dropout layer with a rate of 0.2.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Build a model with a dense layer of 1024 units and a dropout of 0.4, followed by another dense layer with 512 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu"}
{"instruction": "Create a network with 6 layers: a dense layer with 256 units and LeakyReLU, followed by a dropout with rate 0.2, then a dense layer with 128 units and ReLU, a dropout with rate 0.3, and two more dense layers with 64 and 32 units, both using ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout2 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense4 inputs: ~layers[-1] units: 32 activation: ~tf.nn.relu"}
{"instruction": "Design a network with 3 layers: a dense layer with 512 units and ReLU, followed by a dropout layer with rate 0.4, and a final dense layer with 256 units and LeakyReLU.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a neural network with 4 layers: a dense layer with 1024 units and LeakyReLU, followed by a dropout layer with rate 0.2, then a dense layer with 512 units and ReLU, and finally another dense layer with 256 units and LeakyReLU.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu"}
{"instruction": "Build a network with 3 layers: a dense layer with 512 units and LeakyReLU, followed by a dropout layer with rate 0.5, and a dense layer with 128 units and ReLU.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu"}
{"instruction": "Create a network with 2 layers: a dense layer with 256 units and ReLU, followed by a dropout layer with a rate of 0.3.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 3 layers: a dense layer with 1024 units and LeakyReLU activation, followed by a dropout layer with a rate of 0.3, and a final dense layer with 512 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu"}
{"instruction": "Design a network with 4 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with rate 0.4, another dense layer with 256 units and LeakyReLU activation, and a final dense layer with 128 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu"}
{"instruction": "Build a model with 5 layers: a dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with 0.3 rate, another dense layer with 256 units and ReLU activation, and two more dense layers with 128 and 64 units, both with ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense4 inputs: ~layers[-1] units: 64 activation: ~tf.nn.relu"}
{"instruction": "Create a network with 2 layers: a dense layer with 1024 units and ReLU activation, followed by a dropout layer with rate 0.2.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 3 layers: a dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with 0.5 rate, and a final dense layer with 256 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu"}
{"instruction": "Build a model with 6 layers: a dense layer with 256 units and LeakyReLU activation, followed by a dropout layer with rate 0.2, then a dense layer with 128 units and ReLU activation, a dropout layer with rate 0.3, then a dense layer with 64 units and LeakyReLU, and a final dense layer with 32 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout2 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dense arguments: name: dense4 inputs: ~layers[-1] units: 32 activation: ~tf.nn.relu"}
{"instruction": "Create a network with 4 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with rate 0.4, then a dense layer with 256 units and LeakyReLU, and a final dense layer with 128 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu"}
{"instruction": "Create a network with 5 layers: a dense layer with 256 units and LeakyReLU, followed by a dropout layer with 0.3 rate, then a dense layer with 128 units and ReLU, followed by another dropout layer with 0.4 rate, and finally a dense layer with 64 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout2 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu"}
{"instruction": "Build a network with 2 layers: a dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with rate 0.1.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.1 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Create a network with 3 layers: a dense layer with 128 units and ReLU activation, followed by a dropout layer with 0.2 rate, and a final dense layer with 64 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu"}
{"instruction": "Design a network with 4 layers: a dense layer with 256 units and LeakyReLU activation, followed by a dropout layer with rate 0.3, a dense layer with 128 units and ReLU activation, and a final dense layer with 64 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu"}
{"instruction": "Build a model with 5 layers: a dense layer with 1024 units and LeakyReLU activation, followed by a dropout layer with 0.2 rate, another dense layer with 512 units and ReLU activation, then a dropout layer with 0.3 rate, followed by a dense layer with 256 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout2 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 3 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with 0.1 rate, and a final dense layer with 128 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.1 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu"}
{"instruction": "Design a model with 4 layers: a dense layer with 256 units and LeakyReLU activation, followed by a dropout layer with 0.4 rate, then a dense layer with 128 units and ReLU activation, and a final dense layer with 64 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 3 layers: a dense layer with 128 units and ReLU activation, followed by a dropout layer with 0.3 rate, and a final dense layer with 64 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 2 layers: a dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with 0.4 rate.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Design a network with 3 layers: a dense layer with 1024 units and ReLU activation, followed by a dropout layer with 0.2 rate, and a final dense layer with 512 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu"}
{"instruction": "Build a model with 4 layers: a dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with rate 0.5, a dense layer with 256 units and ReLU activation, and a final dense layer with 128 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 3 layers: a dense layer with 1024 units and ReLU activation, followed by a dropout layer with a rate of 0.3, and a final dense layer with 256 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu"}
{"instruction": "Build a network with 4 layers: a dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with 0.4 rate, a dense layer with 128 units and ReLU activation, and a final dense layer with 64 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 2 layers: a dense layer with 256 units and ReLU activation, followed by a dropout layer with rate 0.2.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"]"}
{"instruction": "Design a network with 3 layers: a dense layer with 128 units and LeakyReLU activation, followed by a dropout layer with 0.3 rate, and a final dense layer with 64 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 64 activation: ~tf.nn.relu"}
{"instruction": "Create a model with 5 layers: a dense layer with 1024 units and ReLU activation, followed by a dropout layer with 0.2 rate, another dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with 0.3 rate, and a final dense layer with 256 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout2 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu"}
{"instruction": "Build a network with 4 layers: a dense layer with 512 units and LeakyReLU activation, followed by a dropout layer with 0.5 rate, a dense layer with 256 units and ReLU activation, and a final dense layer with 128 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 3 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with a rate of 0.4, and a final dense layer with 128 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.4 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu"}
{"instruction": "Design a model with 3 layers: a dense layer with 1024 units and LeakyReLU activation, followed by a dropout layer with 0.2 rate, and a final dense layer with 256 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu"}
{"instruction": "Create a network with 3 layers: a dense layer with 128 units and ReLU activation, followed by a dropout layer with 0.3 rate, and a final dense layer with 64 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 128 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 64 activation: ~tf.nn.leaky_relu"}
{"instruction": "Design a model with 3 layers: a dense layer with 512 units and ReLU activation, followed by a dropout layer with a rate of 0.3, and a final dense layer with 128 units and LeakyReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 512 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu"}
{"instruction": "Create a network with 4 layers: a dense layer with 1024 units and ReLU activation, followed by a dropout layer with a rate of 0.5, a dense layer with 256 units and LeakyReLU activation, and a final dense layer with 64 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 1024 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.5 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 256 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.relu"}
{"instruction": "Build a network with 5 layers: a dense layer with 256 units and ReLU activation, followed by a dropout layer with 0.3 rate, another dense layer with 128 units and LeakyReLU activation, followed by a dropout layer with 0.2 rate, and a final dense layer with 64 units and ReLU activation.", "response": "network: - layer_func: ~tf.layers.dense arguments: name: dense1 inputs: ~layers[-1] units: 256 activation: ~tf.nn.relu - layer_func: ~tf.layers.dropout arguments: name: dropout1 inputs: ~layers[-1] rate: 0.3 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense2 inputs: ~layers[-1] units: 128 activation: ~tf.nn.leaky_relu - layer_func: ~tf.layers.dropout arguments: name: dropout2 inputs: ~layers[-1] rate: 0.2 training: ~ph_inputs_dict[\"training\"] - layer_func: ~tf.layers.dense arguments: name: dense3 inputs: ~layers[-1] units: 64 activation: ~tf.nn.relu"}
