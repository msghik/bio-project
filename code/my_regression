"""
regression.py

Main script to load/split the dataset, encode variants, build a TF model,
and perform regression-style training (e.g., fully connected networks).
Now includes optional support for 'plm' encoding from encode.py.

Usage:
  python regression.py --dataset_name avGFP --encoding plm ...
"""

import argparse
import collections
import logging
import os
import os.path
from os.path import join, isdir
import shutil
import sys
import tarfile
import time
import random
import uuid

import numpy as np
import pandas as pd
import shortuuid
import tensorflow as tf

import utils
import split_dataset as sd
import encode as enc
from build_tf_model import build_graph_from_args_dict
from parse_reg_args import get_parser, save_args
import metrics
import constants

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("nn4dms." + __name__)
logger.setLevel(logging.INFO)

def compute_loss(sess, igraph, tgraph, data, set_name, batch_size):
    """
    Computes the average loss over all data batches for the given set_name.
    """
    bg = utils.batch_generator(
        (data["encoded_data"][set_name], data["scores"][set_name]),
        batch_size,
        skip_last_batch=False,
        num_epochs=1,
        shuffle=False
    )

    loss_vals = []
    num_examples_per_batch = []
    for batch_num, batch_data in enumerate(bg):
        ed_batch, scores_batch = batch_data
        num_examples_per_batch.append(len(scores_batch))

        # fill the feed dict with the next batch
        feed_dict = {
            igraph["ph_inputs_dict"]["raw_seqs"]: ed_batch,
            igraph["ph_inputs_dict"]["scores"]: scores_batch
        }

        # compute the loss for this batch
        loss_val = sess.run(tgraph["loss"], feed_dict=feed_dict)
        loss_vals.append(loss_val)

    # return the average loss across each batch
    return np.average(loss_vals, weights=num_examples_per_batch)


def run_eval(sess, args, igraph, tgraph, data, set_name):
    """
    Runs one evaluation against the full epoch of data for set_name.
    Computes predicted vs true scores, then calculates regression metrics.
    """
    bg = utils.batch_generator(
        (data["encoded_data"][set_name], data["scores"][set_name]),
        args.batch_size,
        skip_last_batch=False,
        num_epochs=1,
        shuffle=False
    )

    predicted_scores = np.zeros(data["scores"][set_name].shape)
    true_scores = np.zeros(data["scores"][set_name].shape)

    start = time.time()
    for batch_num, batch_data in enumerate(bg):
        ed_batch, sc_batch = batch_data

        feed_dict = {
            igraph["ph_inputs_dict"]["raw_seqs"]: ed_batch,
            igraph["ph_inputs_dict"]["scores"]: sc_batch
        }

        start_index = batch_num * args.batch_size
        end_index = start_index + args.batch_size

        preds = sess.run(igraph["predictions"], feed_dict=feed_dict)
        predicted_scores[start_index:end_index] = preds
        true_scores[start_index:end_index] = sc_batch

    duration = time.time() - start
    evaluation_dict = metrics.compute_metrics(true_scores, predicted_scores)
    print(f"Evaluation ({set_name} set) completed in {duration:.3f} sec.")
    evaluation_dict["predicted"] = predicted_scores
    evaluation_dict["true"] = true_scores
    return evaluation_dict


def evaluate(sess, args, igraph, tgraph, epoch, data, set_names, summary_writers):
    """
    Perform evaluation on the given sets, printing output & saving to summary writers.
    """
    metrics_ph_dict = tgraph["metrics_ph_dict"]
    summaries_metrics = tgraph["summaries_metrics"]

    evaluations = {}
    for set_name in set_names:
        evaluations[set_name] = run_eval(sess, args, igraph, tgraph, data, set_name)

        # log to tensorboard if we have an epoch number
        if epoch is not None and (set_name in ["train", "tune"]):
            feed_dict = {
                metrics_ph_dict["mse"]: evaluations[set_name]["mse"],
                metrics_ph_dict["pearsonr"]: evaluations[set_name]["pearsonr"],
                metrics_ph_dict["r2"]: evaluations[set_name]["r2"],
                metrics_ph_dict["spearmanr"]: evaluations[set_name]["spearmanr"],
            }
            summary_str = sess.run(summaries_metrics, feed_dict=feed_dict)
            summary_writers[set_name].add_summary(summary_str, epoch)
            summary_writers[set_name].flush()

    print("====================")
    print(evaluations_dict_to_dataframe(evaluations))
    print("====================")
    return evaluations


def evaluations_dict_to_dataframe(evaluations, epoch=None, early=None):
    """
    Convert the evaluations dictionary into a pandas DataFrame.
    Hides 'summary', 'predicted', and 'true' fields.
    """
    p_evaluations = {}
    for set_name, evaluation in evaluations.items():
        evaluation_small = {
            k: v for k, v in evaluation.items()
            if k not in ["summary", "predicted", "true"]
        }
        if epoch is not None:
            evaluation_small["epoch"] = epoch
        if early is not None:
            evaluation_small["early"] = early
        p_evaluations[set_name] = evaluation_small

    sorted_order = ["train", "tune", "test", "stest"]
    metrics_df = pd.DataFrame(p_evaluations).transpose()
    metrics_df.index.rename("set", inplace=True)

    # sort sets in a meaningful order
    metrics_df = metrics_df.sort_index(
        key=lambda sets: [sorted_order.index(s) if s in sorted_order else len(sorted_order) for s in sets]
    )
    return metrics_df


def get_step_display_interval(args, num_train_examples):
    """
    Compute how often to display step info based on fraction of total batches.
    """
    num_batches = num_train_examples // args.batch_size
    step_display_interval = int(num_batches * args.step_display_interval)
    if step_display_interval == 0:
        step_display_interval = 1
    return step_display_interval


def run_training_epoch(sess, args, igraph, tgraph, data, epoch, step_display_interval):
    """
    Train for one epoch (iterate over all training batches).
    Print stats every step_display_interval steps.
    """
    epoch_step_durations = []
    epoch_num_examples_per_batch = []

    interval_step_durations = []
    interval_train_loss_values = []
    interval_num_examples_per_batch = []
    start_step = 1

    bg = utils.batch_generator(
        (data["encoded_data"]["train"], data["scores"]["train"]),
        args.batch_size,
        skip_last_batch=False,
        num_epochs=1
    )

    for step, batch_data in enumerate(bg):
        ed_batch, sc_batch = batch_data
        epoch_num_examples_per_batch.append(len(sc_batch))
        interval_num_examples_per_batch.append(len(sc_batch))

        step += 1
        step_start_time = time.time()

        feed_dict = {
            igraph["ph_inputs_dict"]["raw_seqs"]: ed_batch,
            igraph["ph_inputs_dict"]["scores"]: sc_batch,
            igraph["ph_inputs_dict"]["training"]: True
        }

        _, train_loss_value = sess.run(
            [tgraph["train_op"], tgraph["loss"]],
            feed_dict=feed_dict
        )

        step_duration = time.time() - step_start_time
        epoch_step_durations.append(step_duration)
        interval_step_durations.append(step_duration)
        interval_train_loss_values.append(train_loss_value)

        if step % step_display_interval == 0:
            avg_step_duration = np.average(interval_step_durations, weights=interval_num_examples_per_batch)
            interval_avg_train_loss = np.average(interval_train_loss_values, weights=interval_num_examples_per_batch)
            interval_stat_str = (
                "Epoch {:3} Steps {:4} - {:<4}: "
                "Avg Step = {:.4f}  Avg TLoss = {:.4f}"
            )
            print(interval_stat_str.format(
                epoch, start_step, step,
                avg_step_duration,
                interval_avg_train_loss
            ))
            interval_step_durations = []
            interval_train_loss_values = []
            interval_num_examples_per_batch = []
            start_step = step + 1

    avg_step_duration = np.average(epoch_step_durations, weights=epoch_num_examples_per_batch)
    return avg_step_duration


class LossTracker:
    """
    Tracks training and validation loss over epochs, used for early stopping checks.
    """
    epochs = []
    train_losses = []
    validate_losses = []
    epoch_with_lowest_validate_loss = 1
    lowest_validate_loss = None
    num_epochs_since_lowest_validate_loss = 0
    validate_loss_decrease_thresh = 0

    def __init__(self, mld):
        self.mld = mld

    def add_train_loss(self, epoch, new_train_loss):
        self.epochs.append(epoch)
        self.train_losses.append(new_train_loss)

    def get_train_loss_decrease(self):
        if len(self.train_losses) < 2:
            return 0
        return self.train_losses[-2] - self.train_losses[-1]

    def add_validate_loss(self, epoch, new_validate_loss):
        self.epochs.append(epoch)
        self.validate_losses.append(new_validate_loss)
        self.validate_loss_decrease_thresh = (
            0 if self.lowest_validate_loss is None
            else self.lowest_validate_loss - new_validate_loss
        )

        if (self.lowest_validate_loss is None) or (
            (self.lowest_validate_loss - new_validate_loss) > self.mld
        ):
            self.lowest_validate_loss = new_validate_loss
            self.num_epochs_since_lowest_validate_loss = 0
            self.epoch_with_lowest_validate_loss = epoch
        else:
            self.num_epochs_since_lowest_validate_loss += 1

    def get_validate_loss_decrease(self):
        if len(self.validate_losses) < 2:
            return 0
        return self.validate_losses[-2] - self.validate_losses[-1]


def run_training(data, log_dir, args):
    """
    Main training loop. Builds the TF graph, runs epochs with early stopping,
    saves checkpoints, logs metrics, etc.
    """
    tf.compat.v1.reset_default_graph()
    logger.info(f"setting random seeds py={args.py_rseed}, np={args.np_rseed}, tf={args.tf_rseed}")
    random.seed(args.py_rseed)
    np.random.seed(args.np_rseed)
    tf.compat.v1.set_random_seed(args.tf_rseed)

    # Build graph
    encoded_train_shape = data["encoded_data"]["train"].shape
    igraph, tgraph = build_graph_from_args_dict(args, encoded_data_shape=encoded_train_shape, reset_graph=False)

    step_display_interval = get_step_display_interval(args, len(data["encoded_data"]["train"]))

    max_to_keep = args.early_stopping_allowance + 1 if args.early_stopping else 2
    saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables(), max_to_keep=max_to_keep)

    with tf.compat.v1.Session() as sess:
        summary_writer = tf.compat.v1.summary.FileWriter(log_dir, sess.graph)
        summary_writers = {
            "train": tf.compat.v1.summary.FileWriter(join(log_dir, "train")),
            "tune": tf.compat.v1.summary.FileWriter(join(log_dir, "validation"))
        }

        sess.run(tgraph["init_global"])
        loss_tracker = LossTracker(args.min_loss_decrease)

        logger.info("Starting training loop...")
        for epoch in range(1, args.epochs + 1):
            sys.stdout.flush()
            epoch_start_time = time.time()

            avg_step_duration = run_training_epoch(
                sess, args, igraph, tgraph, data, epoch, step_display_interval
            )

            avg_train_loss = compute_loss(sess, igraph, tgraph, data, "train", args.batch_size)
            loss_tracker.add_train_loss(epoch, avg_train_loss)

            validate_loss = compute_loss(sess, igraph, tgraph, data, "tune", args.batch_size)
            loss_tracker.add_validate_loss(epoch, validate_loss)

            epoch_duration = time.time() - epoch_start_time
            print("====================")
            print(f"= Epoch: {epoch:3}")
            print(f"= Duration: {epoch_duration:.2f}")
            print(f"= Avg Step Duration: {avg_step_duration:.4f}")
            print(f"= Training Loss: {avg_train_loss:.6f}")
            print(f"= Training Loss Decrease (last epoch): {loss_tracker.get_train_loss_decrease():.6f}")
            print(f"= Validation Loss: {validate_loss:.6f}")
            print(f"= Validation Loss Decrease (last epoch): {loss_tracker.get_validate_loss_decrease():.6f}")
            print(f"= Validation Loss Decrease (threshold): {loss_tracker.validate_loss_decrease_thresh:.6f}")
            print(f"= Num Epochs Since Lowest Validation Loss: {loss_tracker.num_epochs_since_lowest_validate_loss}")
            print("====================")

            # Log to TensorBoard
            summary_str = sess.run(
                tgraph["summaries_per_epoch"],
                feed_dict={
                    tgraph["validation_loss_ph"]: validate_loss,
                    tgraph["training_loss_ph"]: avg_train_loss
                }
            )
            summary_writer.add_summary(summary_str, epoch)
            summary_writer.flush()

            # Save checkpoint periodically or last epoch
            if epoch % args.epoch_checkpoint_interval == 0 or epoch == args.epochs:
                save_checkpoint(sess, saver, log_dir, epoch)

            # Evaluate periodically or last epoch
            if epoch % args.epoch_evaluation_interval == 0 or epoch == args.epochs:
                evaluations = evaluate(
                    sess, args, igraph, tgraph, epoch, data,
                    data["encoded_data"].keys(),
                    summary_writers
                )
                if epoch == args.epochs:
                    save_metrics_evaluations(evaluations, log_dir, epoch, early=False, args=args)
                    clean_up_checkpoints(
                        epoch, epoch, log_dir,
                        delete_checkpoints=args.delete_checkpoints,
                        compress_checkpoints=False
                    )
                    if args.compress_everything:
                        compress_everything(log_dir)
                    return evaluations

            # Check for early stopping
            if args.early_stopping and (
                loss_tracker.num_epochs_since_lowest_validate_loss == args.early_stopping_allowance
            ):
                print(
                    "Validation loss hasn't decreased by more than "
                    f"{args.min_loss_decrease} for "
                    f"{args.early_stopping_allowance} epochs in a row."
                )
                print("Training complete.")

                if epoch % args.epoch_checkpoint_interval != 0 and epoch != args.epochs:
                    save_checkpoint(sess, saver, log_dir, epoch)

                if epoch % args.epoch_evaluation_interval != 0 and epoch != args.epochs:
                    evaluate(
                        sess, args, igraph, tgraph, epoch, data,
                        data["encoded_data"].keys(), summary_writers
                    )

                print(f"Loading best model (epoch {loss_tracker.epoch_with_lowest_validate_loss}) and evaluating it.")
                load_checkpoint(sess, saver, log_dir, loss_tracker.epoch_with_lowest_validate_loss)
                evaluations = evaluate(sess, args, igraph, tgraph, None, data, data["encoded_data"].keys(), summary_writers)
                save_metrics_evaluations(
                    evaluations, log_dir,
                    loss_tracker.epoch_with_lowest_validate_loss,
                    early=True, args=args
                )
                clean_up_checkpoints(
                    epoch, loss_tracker.epoch_with_lowest_validate_loss,
                    log_dir, delete_checkpoints=args.delete_checkpoints,
                    compress_checkpoints=False
                )
                if args.compress_everything:
                    compress_everything(log_dir)
                return evaluations


def save_checkpoint(sess, saver, log_dir, epoch):
    checkpoint_file = join(log_dir, 'model.ckpt')
    saver.save(sess, checkpoint_file, global_step=epoch)


def load_checkpoint(sess, saver, log_dir, epoch):
    checkpoint_file = join(log_dir, f"model.ckpt-{epoch}")
    saver.restore(sess, checkpoint_file)


def clean_up_checkpoints(epoch, best_epoch, log_dir, delete_checkpoints=True, compress_checkpoints=True):
    """
    Deletes all checkpoints except the latest and best. Optionally compresses them.
    """
    if delete_checkpoints:
        for fn in os.listdir(log_dir):
            if fn.startswith("model.ckpt") or fn == "checkpoint":
                os.remove(join(log_dir, fn))
    else:
        # keep only latest and best
        for fn in os.listdir(log_dir):
            if fn.startswith("model.ckpt"):
                if f"-{epoch}." not in fn and f"-{best_epoch}." not in fn:
                    os.remove(join(log_dir, fn))

        if compress_checkpoints:
            with tarfile.open(join(log_dir, "models.tar.gz"), "w:gz") as tar:
                for fn in os.listdir(log_dir):
                    if fn.startswith("model.ckpt"):
                        tar.add(join(log_dir, fn), arcname=fn)
            for fn in os.listdir(log_dir):
                if fn.startswith("model.ckpt"):
                    os.remove(join(log_dir, fn))


def compress_everything(log_dir):
    """
    Compresses all output in the log dir except final_evaluation.txt and args.txt
    """
    exclusion = ["final_evaluation.txt", "args.txt"]
    with tarfile.open(join(log_dir, "output.tar.gz"), "w:gz") as tar:
        for fn in os.listdir(log_dir):
            if fn not in exclusion:
                tar.add(join(log_dir, fn), arcname=fn)
    exclusion.append("output.tar.gz")
    for fn in os.listdir(log_dir):
        if fn not in exclusion:
            fp = join(log_dir, fn)
            if os.path.isfile(fp):
                os.remove(fp)
            elif os.path.isdir(fp):
                shutil.rmtree(fp)


def save_scores(evaluation, fn_base):
    np.savetxt(f"{fn_base}_predicted_scores.txt", evaluation["predicted"], fmt="%f")
    np.savetxt(f"{fn_base}_true_scores.txt", evaluation["true"], fmt="%f")


def save_metrics_evaluations(evaluations, log_dir, epoch, early, args):
    """
    Saves metrics for all evaluations to final_evaluation.txt
    and predicted vs true scores to a 'predictions/' dir.
    """
    metrics_df = evaluations_dict_to_dataframe(evaluations, epoch, early)
    metrics_df.to_csv(join(log_dir, "final_evaluation.txt"), sep="\t")

    out_dir = join(log_dir, "predictions")
    utils.mkdir(out_dir)

    for set_name, evaluation in evaluations.items():
        save_scores(evaluation, join(out_dir, set_name))


def log_dir_name(args):
    """
    Constructs a unique log directory name.
    """
    log_dir_str = "log_{}_{}_{}_{}_{}_lr{}_bs{}_{}"
    net_arg = os.path.basename(args.net_file)[:-4] if args.net_file else "nonet"
    ds_arg = args.dataset_name if args.dataset_name else os.path.basename(args.dataset_file)[:-4]

    format_args = [
        args.cluster,
        args.process,
        time.strftime("%Y-%m-%d_%H-%M-%S"),
        ds_arg,
        net_arg,
        args.learning_rate,
        args.batch_size,
        shortuuid.encode(uuid.uuid4())[:8]
    ]
    log_dir = join(args.log_dir_base, log_dir_str.format(*format_args))

    if isdir(log_dir):
        log_dir = log_dir + "_2"
    while isdir(log_dir):
        # if that also exists, increment
        parts = log_dir.split("_")
        if parts[-1].isdigit():
            parts[-1] = str(int(parts[-1]) + 1)
            log_dir = "_".join(parts)
        else:
            log_dir += "_2"
    return log_dir


def main(args):
    """
    Sets up everything: log dir, dataset split, encoding, then calls run_training.
    """
    logger.info(f"software version {utils.__version__}")

    log_dir = log_dir_name(args)
    logger.info(f"log directory is {log_dir}")
    utils.mkdir(log_dir)
    save_args(vars(args), join(log_dir, "args.txt"))

    if args.dataset_name != "":
        dataset_file = constants.DATASETS[args.dataset_name]["ds_fn"]
    else:
        dataset_file = args.dataset_file

    logger.info(f"loading dataset from {dataset_file}")
    ds = utils.load_dataset(ds_fn=dataset_file)

    if args.split_dir != "":
        if isdir(args.split_dir):
            logger.info(f"loading split from {args.split_dir}")
            split = sd.load_split_dir(args.split_dir)
            if isinstance(split, list):
                raise ValueError("this script doesn't support multiple train-size replicates in a single run.")
        else:
            raise FileNotFoundError(f"specified split dir doesn't exist: {args.split_dir}")
    else:
        logger.info(
            f"creating train/test split with tr={args.train_size}, tu={args.tune_size}, "
            f"te={args.test_size}, seed={args.split_rseed}"
        )
        split, _ = sd.train_tune_test(
            ds,
            train_size=args.train_size,
            tune_size=args.tune_size,
            test_size=args.test_size,
            rseed=args.split_rseed
        )

    if "train" not in split:
        raise ValueError("no train set in dataset split.")
    if "tune" not in split:
        raise ValueError(
            "no tune set in dataset split. If you only want train/test, rename your test set to 'tune'."
        )

    logger.info(f"backing up split to log dir {join(log_dir, 'split')}")
    sd.save_split(split, join(log_dir, "split"))

    if args.dataset_name != "":
        wt_aa = constants.DATASETS[args.dataset_name]["wt_aa"]
        wt_ofs = constants.DATASETS[args.dataset_name]["wt_ofs"]
    else:
        wt_aa = args.wt_aa
        wt_ofs = args.wt_ofs

    data = collections.defaultdict(dict)
    data["ds"] = ds

    # Load & encode each subset
    for set_name, idxs in split.items():
        data["idxs"][set_name] = idxs
        data["variants"][set_name] = ds.iloc[idxs]["variant"].tolist()
        data["scores"][set_name] = ds.iloc[idxs]["score"].to_numpy()

        logger.info(f"encoding {set_name} set variants using {args.encoding} encoding")
        encoded_array = enc.encode(
            encoding=args.encoding,
            variants=data["variants"][set_name],
            wt_aa=wt_aa,
            wt_offset=wt_ofs
        )

        # If we used PLM embedding, we get shape [N, seq_len, hidden_dim].
        # We'll do a simple mean pooling or flatten here.
        if "plm" in args.encoding and encoded_array.ndim == 3:
            logger.info(f"Detected plm encoding for {set_name}, shape={encoded_array.shape}")
            # Example: mean pool across axis=1 => shape (N, hidden_dim)
            encoded_array = encoded_array.mean(axis=1)

            # If you prefer flatten: encoded_array = encoded_array.reshape(encoded_array.shape[0], -1)
            # Or max pool: encoded_array = encoded_array.max(axis=1)

        data["encoded_data"][set_name] = encoded_array

    evaluations = run_training(data, log_dir, args)


if __name__ == "__main__":
    parser = get_parser()
    parsed_args = parser.parse_args()

    if parsed_args.dataset_name == "":
        # must supply dataset_file, wt_aa, and wt_ofs
        if parsed_args.dataset_file == "" or parsed_args.wt_aa == "" or parsed_args.wt_ofs == "":
            parser.error(
                "Specify either dataset_name (from constants.py) OR "
                "all of dataset_file, wt_aa, and wt_ofs."
            )

    main(parsed_args)
